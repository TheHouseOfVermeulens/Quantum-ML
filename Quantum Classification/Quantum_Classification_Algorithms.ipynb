{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Quantum_Classification_Algorithms.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMm21qK1Vzjxl6TDsoO9icj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This notebook we discuss quantum classification algorithms implemented in real life. Code examples are presented for different algorithms such as quantum classifiers, support vector machines, and sparse support vector machines."],"metadata":{"id":"GHN3UNPbYqEP"}},{"cell_type":"markdown","source":["Quantum machine learning methods are based on supervised and unsupervised learning. Amplitude modification methods are used in the k-nearest neighbor and clustering quantum algorithms.Quantum matrix computations are used for quantum kernel methods. Quantum kernel methods support vector machines and Gaussian processes. Supervised quantum machine learning is the basis for variational quantum classifier methods. [1]"],"metadata":{"id":"Sa5aUrUqmmBx"}},{"cell_type":"markdown","source":["**Quantum Classifier**"],"metadata":{"id":"RR5Yy1JcpBEO"}},{"cell_type":"code","source":["!pip install qclassify"],"metadata":{"id":"YKJS4a4CogCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dc3T5mPyXohk","executionInfo":{"status":"ok","timestamp":1650955860431,"user_tz":-120,"elapsed":1163,"user":{"displayName":"Werner Vermeulen","userId":"03124494664443879618"}},"outputId":"5018995f-8aa7-4ede-8393-daeb0bc8b5a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["RX(1) 0\n","RX(1) 1\n","CZ 0 1\n","RX(2.0672044712460114) 0\n","RX(1.3311348339721203) 1\n","DECLARE ro BIT[1]\n","MEASURE 0 ro[0]\n","\n"]}],"source":["# Quantum Classifier\n","\n","import qclassify\n","\n","from qclassify.preprocessing import *\n","from qclassify.encoding_circ import *\n","from qclassify.qclassifier import QClassifier as QuantumClassifier\n","from qclassify.proc_circ import *\n","from qclassify.postprocessing import *\n","\n","encoding_opts={\n","                'preprocessing':id_func,        \n","                'encoding_circ':x_product,      \n","        }\n","\n","\n","\n","\n","\n","process_opts={\n","                'proc_circ':layer_xz,   \n","                'postprocessing':{\n","                        'quantum':measure_top, \n","                        'classical':prob_one, \n","                }\n","        }\n","\n","\n","\n","\n","\n","\n","quantum_bits = [0, 1] \n","classify_opts = {\n","                'encoder_options':encoding_opts,\n","                'proc_options':process_opts,\n","        }\n","qCircuit = QuantumClassifier(quantum_bits, classify_opts)\n","\n","\n","\n","\n","\n","circuit_values = [1, 1] \n","params = [2.0672044712460114,          1.3311348339721203] \n","\n","quantum_circuit_prog = qCircuit.circuit(circuit_values, params) \n","\n","print(quantum_circuit_prog)"]},{"cell_type":"markdown","source":["**Variational Quantum Classifier**\n","\n","Variational quantum circuits are used in machine learning. In these circuits, gate parameters are trained using learning methods. The input vectors are the features of the training model. They are equivalent to the quantum system amplitudes. The training method is based on the quantum methods. The classifying solution iterates over the data set, and accuracy is achieved when using the quantum machine learning model and data. The variational quantum classifier algorithm is based on supervised learning. We can use the PennyLane framework for quantum computation and optimization. \n","The package has templates based on different variational quantum circuits used for creating and training machine learning models. Embeddings, layers, subroutines, and state preparations are multiple types of quantum circuits. These circuits are called ansatz circuits. [1]"],"metadata":{"id":"BoG2tZ1spHWB"}},{"cell_type":"code","source":["!pip install pennylane"],"metadata":{"id":"QUY04eLOr9Gt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Variational Quantum Classifier\n","\n","import pennylane as quantumPenny\n","from pennylane import numpy as nump\n","from pennylane.optimize import  NesterovMomentumOptimizer as NMO\n","\n","device = quantumPenny.device('default.qubit', wires=2)\n","\n","\n","def CalcAngles(xval):\n","\n","    beta0_val = 2 * nump.arcsin(nump.sqrt(xval[1]) ** 2 / nump.sqrt(xval[0] ** 2 + xval[1] ** 2 + 1e-12) )\n","    beta1_val = 2 * nump.arcsin(nump.sqrt(xval[3]) ** 2 / nump.sqrt(xval[2] ** 2 + xval[3] ** 2 + 1e-12) )\n","    beta2_val = 2 * nump.arcsin(nump.sqrt(xval[2] ** 2 + xval[3] ** 2) / nump.sqrt(xval[0] ** 2 + xval[1] ** 2 + xval[2] ** 2 + xval[3] ** 2))\n","\n","    return nump.array([beta2_val, -beta1_val / 2, beta1_val / 2, -beta0_val / 2, beta0_val / 2])\n","\n","\n","def CalcState(arr):\n","\n","    quantumPenny.RY(arr[0], wires=0)\n","\n","    quantumPenny.CNOT(wires=[0, 1])\n","    quantumPenny.RY(arr[1], wires=1)\n","    quantumPenny.CNOT(wires=[0, 1])\n","    quantumPenny.RY(arr[2], wires=1)\n","\n","    quantumPenny.PauliX(wires=0)\n","    quantumPenny.CNOT(wires=[0, 1])\n","    quantumPenny.RY(arr[3], wires=1)\n","    quantumPenny.CNOT(wires=[0, 1])\n","    quantumPenny.RY(arr[4], wires=1)\n","    quantumPenny.PauliX(wires=0)\n","\n","\n","def CalcLayer(Warr):\n","\n","    quantumPenny.Rot(Warr[0, 0], Warr[0, 1], Warr[0, 2], wires=0)\n","    quantumPenny.Rot(Warr[1, 0], Warr[1, 1], Warr[1, 2], wires=1)\n","\n","    quantumPenny.CNOT(wires=[0, 1])\n","    \n","\n","@quantumPenny.qnode(device)\n","def FindCircuit(weight, angle=None):\n","\n","    CalcState(angle)\n","    \n","    for W in weight:\n","        CalcLayer(W)\n","\n","    return quantumPenny.expval(quantumPenny.PauliZ(0))\n","\n","\n","def GetVariationalClassifier(var, angle=None):\n","\n","    weight = var[0]\n","    bias_val = var[1]\n","\n","    return FindCircuit(weight, angle=angle) + bias_val\n","\n","\n","def CalcSquareLoss(label, prediction):\n","    loss_val = 0\n","    for l, p in zip(label, prediction):\n","        loss_val = loss_val + (l - p) ** 2\n","    loss_val = loss_val / len(label)\n","\n","    return loss_val\n","\n","\n","def CalcAccuracy(label, prediction):\n","\n","    loss_val = 0\n","    for l, p in zip(label, prediction):\n","        if abs(l - p) < 1e-5:\n","            loss_val = loss_val + 1\n","    loss_val = loss_val / len(label)\n","\n","    return loss_val\n","\n","\n","def CalcCost(weight, feature, label):\n","\n","    prediction = [GetVariationalClassifier(weight, angle=f) for f in feature]\n","\n","    return CalcSquareLoss(label, prediction)\n","\n","\n","data_iris_flowers = nump.loadtxt(\"iris_flower_data.txt\")\n","XVal = data_iris_flowers[:, 0:2]\n","\n","pad = 0.3 * nump.ones((len(XVal), 1))\n","XPad = nump.c_[nump.c_[XVal, pad], nump.zeros((len(XVal), 1)) ] \n","\n","norm = nump.sqrt(nump.sum(XPad ** 2, -1))\n","X_norm = (XPad.T / norm).T  \n","\n","ftrs = nump.array([CalcAngles(x) for x in X_norm])   \n","\n","YVal = data_iris_flowers[:, -1]\n","\n","nump.random.seed(0)\n","num_data = len(YVal)\n","num_train = int(0.75 * num_data)\n","index = nump.random.permutation(range(num_data))\n","feats_train = ftrs[index[:num_train]]\n","YValT = YVal[index[:num_train]]\n","ftrs_val = ftrs[index[num_train:]]\n","Y_val = YVal[index[num_train:]]\n","\n","quantumbits = 2\n","layers = 6\n","varinit = (0.01 * nump.random.randn(layers, quantumbits, 3), 0.0)\n","\n","opt = NMO(0.01)\n","batsize = 5\n","\n","var = varinit\n","for it in range(50):\n","\n","    bat_index = nump.random.randint(0, num_train, (batsize, ))\n","    features_train_batch = feats_train[bat_index]\n","    Ytrainbat = YValT[bat_index]\n","    var = opt.step(lambda v: CalcCost(v, features_train_batch, Ytrainbat), var)\n","\n","    predict_train = [nump.sign(GetVariationalClassifier(var, angle=f)) for f in feats_train]\n","    predict_val = [nump.sign(GetVariationalClassifier(var, angle=f)) for f in ftrs_val]\n","\n","    accuracy_train = CalcAccuracy(YValT, predict_train)\n","    accuracy_val = CalcAccuracy(Y_val, predict_val)\n","\n","    print(\"it value: {:5d} | Cost Features: {:0.7f} | Accuracy training: {:0.7f} | Accuracy for validation: {:0.7f} \"\n","          \"\".format(it+1, CalcCost(var, ftrs, YVal), accuracy_train, accuracy_val))"],"metadata":{"id":"KcMPZSOrqtZg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","**Quantum SVM**\n","\n","The quantum equivalent of SVM uses a feature map to present the quantum circuit information. The kernel concept is used in classification methods. A feature map modified the features. Features are measurable attributes of the subject in context. Classification is about finding the distance between the classes for the input information. The quantum support vector machine is based on a quantum processor for solutions. It is based on supervised learning. Multiclass classifiers are built on quantum support vector machines. A quantum processor has QRAM for handling quantum bits in the memory. [1]"],"metadata":{"id":"oyA2xGK7sz1b"}},{"cell_type":"markdown","source":["**Quantum Sparse Support Vector Machines** \n","\n","There are various techniques such as least squares for simulating quantum sparse support vector machines. The Harrow–Hassidim–Lloyd (HHL) algorithm is used to find a solution for a linear equation–based system to forecast using trained quantum bits. This is a hybrid algorithm of the classical and quantum computing techniques. Regularization is part of the sparse support vector machine The L2 Norm penalty is used as a regularization algorithm, and L1 Norm is better than L2 Norm."],"metadata":{"id":"dJFmXhwntTt-"}},{"cell_type":"code","source":["import renom_q.ml.quantum_svm\n","quantum_svm = renom_q.ml.quantum_svm.QSVM()\n","xval = [[3, 4], [5, 6], [7, 9], [8, 6]]\n","yval = [1, -1, 1, -1]\n","quantum_svm.plot_graph(xval, yval)"],"metadata":{"id":"Leoc461ZvKGy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["References\n","\n","[1] Bhagvan Kommadi"],"metadata":{"id":"-YHIWQ6QzbDH"}}]}
